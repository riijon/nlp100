{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: 機械学習\n",
    "\n",
    "本章では，Bo Pang氏とLillian Lee氏が公開しているMovie Review Dataのsentence polarity dataset v1.0を用い，文を肯定的（ポジティブ）もしくは否定的（ネガティブ）に分類するタスク（極性分析）に取り組む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. データの入手・整形\n",
    "文に関する極性分析の正解データを用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    "\n",
    "1. rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "2. rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "3. 上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "\n",
    "sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nkfコマンドでバイナリからutf8に変換\n",
    "with open('rt-polaritydata/pos', 'r') as f:\n",
    "    sentiment_pos = [\"+1 \" + x[:-1] for x in f.readlines()]\n",
    "with open('rt-polaritydata/neg', 'r') as f:\n",
    "    sentiment_neg = [\"-1 \" + x[:-1] for x in f.readlines()]\n",
    "sentiment_pos[0] = sentiment_pos[0].replace(\"\\ufeff\",\"\")\n",
    "sentiment_neg[0] = sentiment_neg[0].replace(\"\\ufeff\",\"\")\n",
    "sentiment = sentiment_pos + sentiment_neg\n",
    "random.shuffle(sentiment)\n",
    "with open('sentiment.txt', 'w') as f:\n",
    "    f.writelines([x + \"\\n\" for x in sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10661, 5330, 5331)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sentiment.txt', 'r') as f:\n",
    "    sentiment = [x[:-1] for x in f.readlines()]\n",
    "    count = len(sentiment)\n",
    "    count_pos = len([x for x in sentiment if x[:2] == \"+1\"])\n",
    "    count_neg = len([x for x in sentiment if x[:2] == \"-1\"])\n",
    "count, count_pos, count_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. ストップワード\n",
    "英語のストップワードのリスト（ストップリスト）を適当に作成せよ．さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．さらに，その関数に対するテストを記述せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('stopwords.csv', 'r') as f:\n",
    "    stopwords = [x[:-1] for x in f.readlines()]\n",
    "def validate(word):\n",
    "    return word in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate('you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate('aaaaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 素性抽出\n",
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stemming.porter2 import stem\n",
    "wordlist = []\n",
    "for line in sentiment:\n",
    "    wordlist.extend([stem(x) for x in line[:-1].split(\" \")[1:]  if not validate(x)])\n",
    "    column = set(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sixties-styl</th>\n",
       "      <th>power-lunch</th>\n",
       "      <th>translat</th>\n",
       "      <th>unambiti</th>\n",
       "      <th>self-piti</th>\n",
       "      <th>vibranc</th>\n",
       "      <th>fundada</th>\n",
       "      <th>musi</th>\n",
       "      <th>hank</th>\n",
       "      <th>portraitur</th>\n",
       "      <th>...</th>\n",
       "      <th>know-how</th>\n",
       "      <th>twister</th>\n",
       "      <th>jolli</th>\n",
       "      <th>paradiso</th>\n",
       "      <th>ricochet</th>\n",
       "      <th>ghast</th>\n",
       "      <th>akin</th>\n",
       "      <th>gotta</th>\n",
       "      <th>bai</th>\n",
       "      <th>gown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 14549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sixties-styl, power-lunch, translat, unambiti, self-piti, vibranc, fundada, musi, hank, portraitur, super-cool, poster-boy, martinez, pictur, ivi, single-hand, haphazard, sleepless, only-in, design, wretched, nimbl, 1/2-hour, ceremoni, 1930s, peter, toppl, epic, abil, inhabit, affleck, flair, hypothesi, slumber, salut, chancellor, mama, loud, longo, dime-stor, core, child-cent, heart-rate-rais, odyssey, hit-or-miss, audacious-imposs, candy-coat, smokey, bromid, self-satisfact, toler, sl2, jeffrey, are], longest, switchblad, talk-heavi, spoiler, based-on-truth, throb, extrem, famin, #3, decent-enough, low-wattag, miracul, totalment, stoic, a-knock, befal, fixat, [reynolds], coo, screwup, disapprov, scotch, meander, jessica, 2455, galleri, sincera, household, retriev, stun, not-so-big, scan, medal, telegram, pinocchio, empathi, bask, aw, quotat, languish, mangl, buffooneri, italiana, rug, knockout, straight-fac, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 14549 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bow = pd.DataFrame(columns=column)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73. 学習\n",
    "72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章: ベクトル空間法 (II)\n",
    "\n",
    "第10章では，前章に引き続き単語ベクトルの学習に取り組む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90. word2vecによる学習\n",
    "81で作成したコーパスに対してword2vecを適用し，単語ベクトルを学習せよ．さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "with open('81.txt', 'r') as f:\n",
    "    corpus = [f.read().split()]\n",
    "model = Word2Vec(corpus, min_count=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00338275, -0.00363003,  0.00424971, -0.00084945, -0.00309413,\n",
       "        0.00417765, -0.00094517,  0.00034748, -0.00229835, -0.00194972,\n",
       "       -0.00405894,  0.00284101,  0.00090113,  0.00282354,  0.00085077,\n",
       "       -0.00102706,  0.0029569 ,  0.00481773, -0.00333951, -0.00494955,\n",
       "        0.00058217,  0.00133022, -0.002459  , -0.00013361, -0.00170235,\n",
       "       -0.00103415, -0.00399301,  0.00498326, -0.00183375, -0.00091982,\n",
       "       -0.00339756, -0.00176154,  0.00534766, -0.00319785,  0.00101649,\n",
       "        0.0050673 ,  0.00236146, -0.00278086,  0.00312177,  0.00350982,\n",
       "        0.00207829,  0.00453756,  0.00366913,  0.00494404, -0.00013967,\n",
       "        0.00314772, -0.0006055 ,  0.00364694, -0.00178049, -0.00211092,\n",
       "        0.00411564,  0.00199139,  0.0010052 , -0.00368206,  0.00217837,\n",
       "        0.00276143,  0.00230237,  0.00220139, -0.00330923,  0.00092225,\n",
       "        0.00108726,  0.00348572, -0.00146816,  0.00313841, -0.00247652,\n",
       "        0.00102388, -0.00015509,  0.00149702,  0.00440409, -0.00224613,\n",
       "        0.00325529, -0.00155694, -0.00042097, -0.00092562,  0.0037553 ,\n",
       "       -0.00089567, -0.00079276, -0.00552995, -0.00111812, -0.00088868,\n",
       "       -0.0002387 , -0.00251459, -0.00385862,  0.00100948,  0.00428814,\n",
       "        0.00222927,  0.00129372, -0.00302971, -0.00163917,  0.00087054,\n",
       "       -0.00418752,  0.00355323,  0.00177583, -0.00014012,  0.0049159 ,\n",
       "        0.00260777, -0.00214392,  0.00094116,  0.00202614,  0.00209304], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 86 ベクトル表示\n",
    "model['United_States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10478716769041592"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 87 コサイン類似度 \n",
    "model.similarity(\"United_States\", \"U.S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mainly', 0.4305400550365448),\n",
       " ('double-helical', 0.42140504717826843),\n",
       " ('leaked', 0.41813257336616516),\n",
       " ('Alentejo', 0.415567547082901),\n",
       " ('reconciling', 0.40542083978652954),\n",
       " ('fermée', 0.40274858474731445),\n",
       " ('Wickersham', 0.40134602785110474),\n",
       " ('Krummenau', 0.40052151679992676),\n",
       " ('Wan', 0.39969754219055176),\n",
       " ('Izyaslavich', 0.396148145198822)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 88 類似度 top10\n",
    "model.most_similar(positive=['England'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"Afrikakorps\"', 0.4499354064464569),\n",
       " ('Krom', 0.44138073921203613),\n",
       " ('midnight.', 0.42218679189682007),\n",
       " ('ligature', 0.4191877245903015),\n",
       " ('Seddu', 0.4180520176887512),\n",
       " ('-Don', 0.41638216376304626),\n",
       " ('93-page', 0.4152953028678894),\n",
       " ('Publishings', 0.41490593552589417),\n",
       " ('Ostrivky', 0.4095712900161743),\n",
       " ('gunniella', 0.4037185609340668)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 89 アナロジー top10\n",
    "model.most_similar(positive=['Spain', 'Athens'], negative=['Madrid'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 91. アナロジーデータの準備\n",
    "単語アナロジーの評価データをダウンロードせよ．このデータ中で\": \"で始まる行はセクション名を表す．例えば，\": capital-common-countries\"という行は，\"capital-common-countries\"というセクションの開始を表している．ダウンロードした評価データの中で，\"family\"というセクションに含まれる評価事例を抜き出してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('questions-words.txt', 'r') as f:\n",
    "    sections = f.read().split(':')\n",
    "    for section in sections:\n",
    "        if 'family' in section.split('\\n')[0]:\n",
    "            family = \"\\n\".join(section.split('\\n')[1:])\n",
    "with open('family.txt', 'w') as f:\n",
    "    f.write(family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 92. アナロジーデータへの適用\n",
    "91で作成した評価データの各事例に対して，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('family.txt', 'r') as f:\n",
    "    family = [x[:-1].split() for x in f.readlines()]\n",
    "len(family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_contain(words):\n",
    "    for w in words:\n",
    "        if not w in model:\n",
    "            return False \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('92.txt', 'w') as f:\n",
    "    for line in family:\n",
    "        c1, c2, c3, c4 = line\n",
    "        nomatch = model.doesnt_match(line)\n",
    "        if check_contain(line):\n",
    "            w, p = model.most_similar(positive=[c2, c3], negative=[c1], topn=1)[0]\n",
    "            line = str(w) + \"\\t\" + str(p) + \"\\n\"\n",
    "        else:\n",
    "            line = \"NoMatch\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 93. アナロジータスクの正解率の計算\n",
    "92で作ったデータを用い，各モデルのアナロジータスクの正解率を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count93 = 0\n",
    "with open('92.txt', 'r') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        w = line[:-1].split()[0]\n",
    "        ans_w = family[i][3]\n",
    "        if w != \"NoMatch\" and w == ans_w:\n",
    "            count93 += 1\n",
    "            print(\"match!!!!!\")\n",
    "count93 / len(family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 94. WordSimilarity-353での類似度計算\n",
    "The WordSimilarity-353 Test Collectionの評価データを入力とし，1列目と2列目の単語の類似度を計算し，各行の末尾に類似度の値を追加するプログラムを作成せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "class W2V:\n",
    "    def __init__(self):\n",
    "        with open('Xpca.csv', 'r') as f:\n",
    "            self.Xpca = np.array([np.array(line[:-1].split(','), dtype=np.float64) for line in f.readlines()])\n",
    "        with open('dictionary.txt', 'r') as f:\n",
    "            self.dictionary = np.array([x[:-1] for x in f.readlines()])\n",
    "\n",
    "    def vec(self, word):\n",
    "        dictionary = self.dictionary\n",
    "        i = np.where(dictionary == word)[0][0]\n",
    "        return self.Xpca[i].reshape(1, -1)\n",
    "\n",
    "    def get_word_cs(self, word1, word2):\n",
    "        return cosine_similarity(self.vec(word1), self.vec(word2))[0][0]\n",
    "    \n",
    "    def get_top10_w(self, word):\n",
    "        lst=[]\n",
    "        for i, w in enumerate(self.Xpca[:1000]):\n",
    "            cs = cosine_similarity(self.vec(word), w.reshape(1, -1))[0][0]\n",
    "            lst.append((self.dictionary[i], cs))\n",
    "        return pd.DataFrame(lst).sort_values(1, ascending=False).head(10)\n",
    "    \n",
    "    def get_top10_v(self, vec):\n",
    "        lst=[]\n",
    "        for i, w in enumerate(self.Xpca[:1000]):\n",
    "            cs = cosine_similarity(vec, w.reshape(1, -1))\n",
    "            lst.append((self.dictionary[i], cs[0][0]))\n",
    "        df = pd.DataFrame(lst)\n",
    "        return df.sort_values(1, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v = W2V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>events</td>\n",
       "      <td>0.806443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>areas</td>\n",
       "      <td>0.752089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>without</td>\n",
       "      <td>0.476834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>3.84%</td>\n",
       "      <td>0.143077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>seceded</td>\n",
       "      <td>0.115013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>propitious</td>\n",
       "      <td>0.096278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Mosson</td>\n",
       "      <td>0.094246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>own</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>PEN/Laura</td>\n",
       "      <td>0.077884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Town.</td>\n",
       "      <td>0.067260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "47       events  0.806443\n",
       "616       areas  0.752089\n",
       "736     without  0.476834\n",
       "207       3.84%  0.143077\n",
       "63      seceded  0.115013\n",
       "292  propitious  0.096278\n",
       "177      Mosson  0.094246\n",
       "301         own  0.089700\n",
       "127   PEN/Laura  0.077884\n",
       "174       Town.  0.067260"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_top10_w(\"England\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('combined.csv', 'r') as f:\n",
    "    combined = f.readlines()[1:]\n",
    "with open('94.csv', 'w') as f:\n",
    "     for line in combined:\n",
    "            w1, w2, p = line.split(',')\n",
    "            if check_contain([w1, w2]):\n",
    "                s90 = model.similarity(w1, w2)\n",
    "            else:\n",
    "                s90 = 0\n",
    "            l = line[:-1] + ',' + str(s90) + '\\n'\n",
    "            f.write(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95. WordSimilarity-353での評価\n",
    "94で作ったデータを用い，各モデルが出力する類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96. 国名に関するベクトルの抽出\n",
    "word2vecの学習結果から，国名に関するベクトルのみを抜き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 97. k-meansクラスタリング\n",
    "96の単語ベクトルに対して，k-meansクラスタリングをクラスタ数k=5k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 98. Ward法によるクラスタリング\n",
    "96の単語ベクトルに対して，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. t-SNEによる可視化\n",
    "96の単語ベクトルに対して，ベクトル空間をt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

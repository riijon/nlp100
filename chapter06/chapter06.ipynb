{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"nlp.txt\", 'r') as f:\n",
    "    nlp = \" \".join(f.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50. 文区切り\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = '[.|;|:|?|!]\\s+([A-Z])'\n",
    "sentences = []\n",
    "split_line = re.split(p, nlp)\n",
    "sentences.append(split_line[0])\n",
    "for i in range(1, len(split_line), 2):\n",
    "    sentences.append(split_line[i] + split_line[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('50.txt', 'w') as f:\n",
    "    for s in sentences:\n",
    "        f.write(s) \n",
    "        f.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('51.txt', 'w') as f:\n",
    "    for s in sentences:\n",
    "        for w in s.split(' '):\n",
    "            if w: \n",
    "                f.write(w)\n",
    "                f.write('\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 52. ステミング\n",
    "\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stemming.porter2 import stem\n",
    "with open('51.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "with open('52.txt', 'w') as f:\n",
    "    for w in words:\n",
    "        word = w[:-1]\n",
    "        line = word + \"\\t\" + stem(word) + \"\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 53. Tokenization\n",
    "Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file nlp.txt\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('nlp.txt.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('53.txt', 'w') as f:\n",
    "    for child in root.iter('word'):\n",
    "        f.write(child.text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 54. 品詞タグ付け\n",
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ['word', 'lemma', 'CharacterOffsetBegin', 'CharacterOffsetEnd', 'POS', 'NER', 'Speaker']\n",
    "with open('54.txt', 'w') as f:\n",
    "    for child in root.iter('token'):\n",
    "        word = child.find('word').text\n",
    "        lemma = child.find('lemma').text\n",
    "        pos = child.find('POS').text\n",
    "        line = word + \"\\t\" + lemma +\"\\t\" + pos\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 55. 固有表現抽出\n",
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('55.txt', 'w') as f:\n",
    "    for child in root.iter('token'):\n",
    "        if child.find('NER').text == \"PERSON\":\n",
    "            word = child.find('word').text\n",
    "            f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 56. 共参照解析\n",
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corefereneces =[[m for m in child.iter('mention')] for child in root.iter('coreference')]\n",
    "sentences = [[x.find('word').text for x in child.iter('token')] for child in root.find('document/sentences').iter('sentence')]\n",
    "for c in corefereneces:\n",
    "    representative_mention  = c[0]\n",
    "    representative_text = representative_mention.find(\"text\").text\n",
    "    for mention in c[1:]:\n",
    "        sentence = int(mention.find(\"sentence\").text) - 1\n",
    "        start = int(mention.find(\"start\").text) - 1\n",
    "        end = int(mention.find(\"end\").text) - 1\n",
    "        text = representative_text + \"(\" + mention.find(\"text\").text + \")\"\n",
    "        sentences[sentence][start] = text\n",
    "        sentences[sentence][start+1:end] = [\"\" for x in range(end - start - 1)]\n",
    "with open('56.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join([\" \".join([x for x in sentence if x != \"\"]) for sentence in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 57. 係り受け解析\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "edges = []\n",
    "collapsed_dependencies = \\\n",
    "    [sentence.find('dependencies[@type=\"collapsed-dependencies\"]') for sentence in root.find('document/sentences').iter('sentence')]\n",
    "# for i in range(len(collapsed_dependencies)):\n",
    "# 全部やると時間かかるので一部のみ\n",
    "for i in range(10):\n",
    "    for dep in collapsed_dependencies[i].iter('dep'):\n",
    "        governor = str(i) + \" \" + dep.find('governor').attrib['idx'] + \" \" + dep.find('governor').text\n",
    "        dependent = str(i) + \" \" + dep.find('dependent').attrib['idx'] + \" \" + dep.find('dependent').text\n",
    "        edges.append((dependent, governor))\n",
    "g=pydot.graph_from_edges(edges, directed=True)\n",
    "g.write_jpeg('57.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 58. タプルの抽出\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "\n",
    "+ 述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "+ 主語: 述語からnsubj関係にある子（dependent）\n",
    "+ 目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('58.txt', 'w') as f:\n",
    "    for collapsed_dependencie in collapsed_dependencies:\n",
    "        for dep in collapsed_dependencie.iter('dep'):\n",
    "            type = dep.attrib['type']\n",
    "            if type == 'nsubj':\n",
    "                predicate = dep.find('governor').text\n",
    "                predicate_id = dep.find('governor').attrib['idx']\n",
    "                dependants = [x for x in collapsed_dependencie.findall('dep') if x.find('governor').attrib['idx'] == str(predicate_id)]\n",
    "                subject = [d.find('dependent').text for d in dependants if d.attrib['type'] == 'nsubj']\n",
    "                object = [d.find('dependent').text for d in dependants if d.attrib['type'] == 'dobj']\n",
    "                if bool(subject) and bool(object):\n",
    "                    line = subject[0] + \"\\t\" + predicate +\"\\t\" + object[0]\n",
    "                    f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 59. S式の解析\n",
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parses =[x.text for x in root.iter('parse')]\n",
    "with open('59.txt', 'w') as f:\n",
    "    for parse in parses:\n",
    "        parse_list = parse.split(\" \")\n",
    "        np_ids =[i for i in range(len(parse_list)) if parse_list[i] == \"(NP\"]\n",
    "        for i in np_ids:\n",
    "            left, right, np = 0, 0, []\n",
    "            for w in parse_list[i + 1:]:\n",
    "                if \"(\" in w:\n",
    "                    left +=1 \n",
    "                if \")\" in w:\n",
    "                    right += len([x for x in list(w) if x == ')'])\n",
    "                np.append(w)\n",
    "                if left < right:\n",
    "                    break\n",
    "            line = \" \".join([x.replace(')', '') for x in np if ')' in x])\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

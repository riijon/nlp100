{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pydot\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# cabocha -f1 neko.txt > neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('neko.txt.cabocha') as f:\n",
    "    source = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    " 形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）を\n",
    " メンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，\n",
    " 各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self):\n",
    "        self.surface = \"\"\n",
    "        self.base = \"\"\n",
    "        self.pos = \"\"\n",
    "        self.pos1 = \"\"\n",
    "    def set(self, line):\n",
    "        line = line.split('\\t')\n",
    "        surface = line[0]\n",
    "        line = line[1][:-1].split(\",\")\n",
    "        self.surface = surface\n",
    "        self.base = line[6]\n",
    "        self.pos = line[0]\n",
    "        self.pos1 = line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neko = []\n",
    "for line in source:\n",
    "    if '* 0' in line:\n",
    "        sentence = []\n",
    "    if 'EOS' in line:\n",
    "        neko.append(sentence)\n",
    "    if '\\t' in line:\n",
    "        val = Morph()\n",
    "        val.set(line)\n",
    "        sentence.append(val)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　\n",
      "吾輩\n",
      "は\n",
      "猫\n",
      "で\n",
      "ある\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "for morph in neko[2]:\n",
    "    print(morph.surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "空白\n",
      "代名詞\n",
      "係助詞\n",
      "一般\n",
      "*\n",
      "*\n",
      "句点\n"
     ]
    }
   ],
   "source": [
    "for morph in neko[2]:\n",
    "    print(morph.pos1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    " 40に加えて，文節を表すクラスChunkを実装せよ．\n",
    " このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）を\n",
    " メンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，\n",
    " １文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.id = 0 \n",
    "        self.morphs = [] \n",
    "        self.dst = 0 \n",
    "        self.srcs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_srcs(sentence):\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst != -1:\n",
    "            sentence[chunk.dst].srcs.append(chunk.id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neko = []\n",
    "for line in source:\n",
    "    if '* ' in line:\n",
    "        id = int(line.split(\" \")[1])\n",
    "        if id == 0:\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(chunk)\n",
    "        chunk = Chunk()\n",
    "        chunk.id = id \n",
    "        chunk.dst = int(line.split(\" \")[2][:-1])\n",
    "    if 'EOS' in line:\n",
    "        if chunk:\n",
    "            sentence.append(chunk)\n",
    "            set_srcs(sentence)\n",
    "        neko.append(sentence)\n",
    "        sentence = []\n",
    "        chunk = None\n",
    "    if '\\t' in line:\n",
    "        val = Morph()\n",
    "        val.set(line)\n",
    "        chunk.morphs.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "吾輩\n",
      "は\n",
      "2\n",
      "ここ\n",
      "で\n",
      "3\n",
      "始め\n",
      "て\n",
      "4\n",
      "人間\n",
      "という\n",
      "5\n",
      "もの\n",
      "を\n",
      "-1\n",
      "見\n",
      "た\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "for chunk in neko[7]:\n",
    "    print(chunk.dst)\n",
    "    for morph in chunk.morphs:\n",
    "        print(morph.surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk2word(chunk):\n",
    "    return \"\".join([x.surface for x in chunk.morphs if not (\"。\" in x.surface) or (\"、\" in x.surface)])\n",
    "\n",
    "def showSrcAndDst(sentence): \n",
    "    for chunk in sentence:\n",
    "        if chunk.dst != -1:\n",
    "            src = chunk2word(chunk)\n",
    "            dst = chunk2word(sentence[chunk.dst])\n",
    "            return src + \"\\t\" + dst\n",
    "\n",
    "with open('42.txt', 'w') as f:           \n",
    "    for sentence in neko:\n",
    "        word = showSrcAndDst(sentence)\n",
    "        if word:\n",
    "            f.write(word)\n",
    "            f.write(\"\\n\")\n",
    "# 9530件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showNounAndVerb(sentence): \n",
    "    def checkPos(chunk, pos):\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst != -1:\n",
    "            chunk2 = sentence[chunk.dst]\n",
    "            if checkPos(chunk, \"名詞\") and checkPos(chunk2, \"動詞\"):\n",
    "                src = chunk2word(chunk)\n",
    "                dst = chunk2word(chunk2)\n",
    "                return src + \"\\t\" + dst\n",
    "\n",
    "with open('43.txt', 'w') as f:           \n",
    "    for sentence in neko:\n",
    "        word = showNounAndVerb(sentence)\n",
    "        if word:\n",
    "            f.write(word)\n",
    "            f.write(\"\\n\")\n",
    "# 8079件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．\n",
    "可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．\n",
    "また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getSrcAndDst(sentence):\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst != -1:\n",
    "            src = chunk2word(chunk)\n",
    "            dst = chunk2word(sentence[chunk.dst])\n",
    "            return (src, dst)\n",
    "\n",
    "edges = []\n",
    "for sentence in neko[:50]:\n",
    "    edge = getSrcAndDst(sentence)\n",
    "    if edge:\n",
    "        edges.append(edge)\n",
    "g=pydot.graph_from_edges(edges, directed=True)\n",
    "g.write_jpeg('44.jpg')\n",
    "# 全部やると時間かかるので一部のみ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "+ 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "+ 述語に係る助詞を格とする\n",
    "+ 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getVerbAndCase(chunk, sentence): \n",
    "    def checkPos(chunk, pos):\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if checkPos(chunk, \"動詞\"):\n",
    "        case = []\n",
    "        for id in chunk.srcs:\n",
    "            for morph in sentence[id].morphs:\n",
    "                if morph.pos == \"助詞\":\n",
    "                    case.append(morph.surface)\n",
    "            verb = chunk.morphs[0].base\n",
    "            if len(case) > 0:\n",
    "                return verb + \" \" + \" \".join(sorted(case))\n",
    "\n",
    "with open('45.txt', 'w') as f:           \n",
    "    for sentence in neko:\n",
    "        for chunk in sentence:\n",
    "            word = getVerbAndCase(chunk, sentence)\n",
    "            if word:\n",
    "                f.write(word)\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# unixコマンド\n",
    "\n",
    "`sort 45.txt | uniq -c | sort`\n",
    "\n",
    "```\n",
    "    196 なる に\n",
    "    221 見る て\n",
    "    249 ある が\n",
    "    263 思う と\n",
    "    268 する を\n",
    "    615 云う と\n",
    "```\n",
    "\n",
    "`grep する 45.txt | sort | uniq -c | sort`\n",
    "\n",
    "```\n",
    "    107 する て\n",
    "    113 する が\n",
    "    126 する に\n",
    "    138 する と\n",
    "    268 する を\n",
    "```\n",
    "\n",
    "`grep 見る 45.txt | sort | uniq -c | sort`\n",
    "\n",
    "```\n",
    "     25 見る と\n",
    "     29 見る が\n",
    "     38 見る は\n",
    "     42 見る から\n",
    "    102 見る を\n",
    "    221 見る て\n",
    "```\n",
    "\n",
    "`grep 与える 45.txt | sort | uniq -c | sort`\n",
    "\n",
    "```\n",
    "    　1 与える に は\n",
    "      1 与える ば\n",
    "      2 与える て\n",
    "      2 与える は\n",
    "      4 与える に\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "+ 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "+ 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getVerbAndCaseFrame(chunk, sentence): \n",
    "    def checkPos(chunk, pos):\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == pos:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if checkPos(chunk, \"動詞\"):\n",
    "        case = []\n",
    "        for id in chunk.srcs:\n",
    "            for morph in sentence[id].morphs:\n",
    "                if morph.pos == \"助詞\":\n",
    "                    case.append(morph.surface)\n",
    "            if len(case) > 0:\n",
    "                verb = chunk.morphs[0].base\n",
    "                frame = [chunk2word(sentence[i]) for i in chunk.srcs]\n",
    "                return verb + \" \" + \" \".join(sorted(case)) + \" \" + \" \".join(sorted(frame))\n",
    "\n",
    "with open('46.txt', 'w') as f:           \n",
    "    for sentence in neko:\n",
    "        for chunk in sentence:\n",
    "            word = getVerbAndCaseFrame(chunk, sentence)\n",
    "            if word:\n",
    "                f.write(word)\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
